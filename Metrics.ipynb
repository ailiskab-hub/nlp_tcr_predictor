{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0266c4fa-3ad6-4a78-9acf-f94cb7f48fae",
   "metadata": {},
   "source": [
    "Дальше хочется сравнить две модели и посмотреть, какая из них лучше/хуже справляется с задачей (метрики f1-score, precision, recall). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "608b73ee-6b32-4113-949f-26ca3e3140fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: Tesla V100S-PCIE-32GB\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "from my_model import CustomModel, train_per_protein, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6b2da37-0534-4cb0-bab8-f9ddcaf2ff09",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "# torch.manual_seed(seed_val)\n",
    "# torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40f36751-996f-41f2-8b59-d639e8a77569",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(tokenizer,seqs,seqs_labels):\n",
    "    tokenized = tokenizer(seqs, max_length=25, padding = 'longest', truncation=True)\n",
    "    dataset = Dataset.from_dict(tokenized)\n",
    "    dataset = dataset.add_column(\"labels\", seqs_labels)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3dea025-acf3-4894-bf7b-cd9de2a9752d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cdr3aa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C A S S L A G A G P Y N E Q F F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C A S S F Q G T S S Y Y T F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C A I G I A G G N E Q F F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C A S S L A A S G G R Y T Q Y F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C A L A S Y G N N R L A F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17218</th>\n",
       "      <td>C L L D R V S N Y Q L I W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17219</th>\n",
       "      <td>C A S S P Q G V T G E L F F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17220</th>\n",
       "      <td>C A S S L L P G G A S P D T Q Y F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17221</th>\n",
       "      <td>C A S S Q D R D L L Q E T Q Y F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17222</th>\n",
       "      <td>C A V D P T G G G N K L T F</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17223 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  cdr3aa\n",
       "0        C A S S L A G A G P Y N E Q F F\n",
       "1            C A S S F Q G T S S Y Y T F\n",
       "2              C A I G I A G G N E Q F F\n",
       "3        C A S S L A A S G G R Y T Q Y F\n",
       "4              C A L A S Y G N N R L A F\n",
       "...                                  ...\n",
       "17218          C L L D R V S N Y Q L I W\n",
       "17219        C A S S P Q G V T G E L F F\n",
       "17220  C A S S L L P G G A S P D T Q Y F\n",
       "17221    C A S S Q D R D L L Q E T Q Y F\n",
       "17222        C A V D P T G G G N K L T F\n",
       "\n",
       "[17223 rows x 1 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = pd.read_csv('test_dataset.csv')\n",
    "ans = pd.read_csv('ans.csv')\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cde60eeb-e5d5-4e39-bb2a-5aee084f9660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17218</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17219</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17220</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17221</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17222</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17223 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       type\n",
       "0         1\n",
       "1         1\n",
       "2         1\n",
       "3         1\n",
       "4         0\n",
       "...     ...\n",
       "17218     0\n",
       "17219     1\n",
       "17220     1\n",
       "17221     1\n",
       "17222     0\n",
       "\n",
       "[17223 rows x 1 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2ee40bc-27a2-46ad-a649-e5e74857f4d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at wukevin/tcr-bert and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([45, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([45]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at Rostlab/prot_t5_xl_uniref50 were not used when initializing T5EncoderModel: ['decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.final_layer_norm.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.embed_tokens.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'lm_head.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProtT5_Classfier\n",
      "Trainable Parameter: 1209193474\n",
      "ProtT5_LoRA_Classfier\n",
      "Trainable Parameter: 3559426\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TCRbert = load_model(\"./TCR_bert_finetuned.pth\", mod_type='TCR-bert', num_labels=2)\n",
    "TCRbert.maximun_len = 25\n",
    "\n",
    "tokenizer, Prottrans = load_model(\"./PT5_GB1_finetuned.pth\", num_labels=2, mixed=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947b0d18-7576-4362-8818-43b3d350601e",
   "metadata": {},
   "source": [
    "## f1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "429e01cf-f766-4dff-a4f1-32735df1a57d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/akabalina/.conda/envs/ft/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total f1_score: 0.99990\n"
     ]
    }
   ],
   "source": [
    "TCRbert.to(device)\n",
    "pred_labels = TCRbert.predict(test_dataset['cdr3aa'])\n",
    "\n",
    "f1_sc_tcr = f1_score(ans, pred_labels)\n",
    "print('Total f1_score: %.5f' % f1_sc_tcr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3af57a4-361a-4353-a565-5dc6eb0c96b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total f1_score: 0.99966\n"
     ]
    }
   ],
   "source": [
    "Prottrans.to(device)\n",
    "\n",
    "test_set=create_dataset(tokenizer, list(test_dataset['cdr3aa']),list(ans['type']))\n",
    "test_set = test_set.with_format(\"torch\", device=device)\n",
    "\n",
    "test_dataloader = DataLoader(test_set, batch_size=8, shuffle=False)\n",
    "\n",
    "Prottrans.eval()\n",
    "\n",
    "pred_labels_2 = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        #add batch results(logits) to predictions\n",
    "        pred_labels_2 += Prottrans(input_ids, attention_mask=attention_mask).logits.tolist()\n",
    "pred_labels_2= [item.argmax() for item in np.array(pred_labels_2)]\n",
    "\n",
    "f1_sc_prt = f1_score(ans, pred_labels_2)\n",
    "print('Total f1_score: %.5f' % f1_sc_prt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d285c1-11d8-4d57-bb7b-2c90400d33ea",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cddc89c2-9066-4aa3-8a2f-7257a3d867b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Accuracy TCRbert: 0.99988\n",
      "Total Accuracy Prottrans: 0.99959\n"
     ]
    }
   ],
   "source": [
    "acc_tcr = accuracy_score(ans, pred_labels)\n",
    "print('Total Accuracy TCRbert: %.5f' % acc_tcr)\n",
    "\n",
    "acc_prt = accuracy_score(ans, pred_labels_2)\n",
    "print('Total Accuracy Prottrans: %.5f' % acc_prt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3be31ea-1280-48f9-8163-95e90a6efff1",
   "metadata": {},
   "source": [
    "## precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2b3ee125-023c-4456-87b7-414e854b50a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total precision TCRbert: 0.99990\n",
      "Total Accuracy Prottrans: 0.99981\n"
     ]
    }
   ],
   "source": [
    "precision_tcr = precision_score(ans, pred_labels)\n",
    "precision_prttr = precision_score(ans, pred_labels_2)\n",
    "\n",
    "print('Total precision TCRbert: %.5f' % precision_tcr)\n",
    "print('Total Accuracy Prottrans: %.5f' % precision_prttr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eceea899-a320-4afa-b979-a3faa040e86e",
   "metadata": {},
   "source": [
    "## recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1b0b84b6-7ec9-418c-a31a-d1406c2f778a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total recall TCRbert: 0.99990\n",
      "Total recall Prottrans: 0.99951\n"
     ]
    }
   ],
   "source": [
    "recall_tcr = recall_score(ans, pred_labels)\n",
    "recall_prt = recall_score(ans, pred_labels_2)\n",
    "\n",
    "print('Total recall TCRbert: %.5f' % recall_tcr)\n",
    "print('Total recall Prottrans: %.5f' % recall_prt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c3fa9f-b475-433f-9477-a6cd82f0bb5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c624990f-d624-4871-bfaf-34396699ae9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89a82b0-f841-4b17-aa93-7c7303bf87ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-ft]",
   "language": "python",
   "name": "conda-env-.conda-ft-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
